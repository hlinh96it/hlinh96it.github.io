---
title: Thuáº­t toÃ¡n Gradient Descent (GD) vá»›i Python
author: hoanglinh
categories: [Machine Learning, Feature Engineering]
tags: [dimensional reduction]
math: true
img_path: posts_media/2023-02-02-posts/
---

Trong bÃ i viáº¿t nÃ y, chÃºng ta sáº½ cÃ¹ng nhau tÃ¬m hiá»ƒu vá» Partial Least Squares (PLS). ÄÃºng nhÆ° tÃªn gá»i cá»§a nÃ³, PLS cÃ³ liÃªn quan Ä‘áº¿n Ordinary Least Squares, nÃ³ lÃ  má»™t phÆ°Æ¡ng phÃ¡p toÃ¡n há»c cÆ¡ báº£n Ä‘á»ƒ tÃ¬m ra Ä‘Æ°á»ng fitting linear regression.

DÃ nh cho cÃ¡c báº¡n má»›i lÃ m quen vá»›i Linear regression, cÃ¡c báº¡n cÃ³ thá»ƒ tÃ¬m cÃ¡c bÃ i viáº¿t liÃªn quan: [thuáº­t toÃ¡n machine learning linear regression](https://machinelearningcoban.com/2016/12/28/linearregression/) vÃ  cÃ¡c [giáº£ Ä‘á»‹nh Ä‘á»‘i vá»›i data cá»§a linear regression](https://www.notion.so/deba871a26d34c67bac023a447fab221).

## Partial Least Squares Ä‘á»‘i vá»›i hiá»‡n tÆ°á»£ng Ä‘a cá»™ng tuyáº¿n (multi-collinearity)

Hiá»ƒu vá» cÆ¡ báº£n, hiá»‡n tÆ°á»£ng Ä‘a cá»™ng tuyáº¿n trong linear regression lÃ  hiá»‡n tÆ°á»£ng cÃ¡c biáº¿n phá»¥ thuá»™c tuyáº¿n tÃ­nh láº«n nhau, thá»ƒ hiá»‡n dÆ°á»›i hÃ m sá»‘ vÃ  vi pháº¡m giáº£ Ä‘á»‹nh sá»‘ 5 cá»§a mÃ´ hÃ¬nh linear regression. Báº¡n Ä‘á»c quan tÃ¢m cÃ³ thá»ƒ tham kháº£o bÃ i viáº¿t vá» [há»‡ quáº£ cá»§a Ä‘a cá»™ng tuyáº¿n vá»›i mÃ´ hÃ¬nh linear regression](https://www.notion.so/6cb85c40ecdf4a3abbd01f634f1c3508).

![Nguá»“n: Xá»­ lÃ½ sá»‘ liá»‡u SPSS](da-cong-tuyen.png)_Nguá»“n: Xá»­ lÃ½ sá»‘ liá»‡u SPSS_

Má»¥c tiÃªu cá»§a linear regression lÃ  mÃ´ phá»ng láº¡i má»‘i liÃªn há»‡ phá»¥ thuá»™c giá»¯a target vÃ  má»™t hoáº·c nhiá»u variables (hay cÃ²n gá»i lÃ  inputs). MÃ´ hÃ¬nh OLS cÃ³ thá»ƒ cho ra káº¿t quáº£ tá»‘t náº¿u cÃ¡c variable khÃ´ng vi pháº¡m [cÃ¡c giáº£ Ä‘á»‹nh](https://www.notion.so/deba871a26d34c67bac023a447fab221). Tuy nhiÃªn, thá»±c táº¿ lÃ  cÃ¡c inputs cÃ³ thá»ƒ cÃ³ má»‘i liÃªn há»‡ vá»›i nhau vÃ  tá»« Ä‘Ã³, káº¿t quáº£ cá»§a OLS khÃ´ng cÃ²n Ä‘Ã¡ng tin cáº­y ná»¯a.

PLS Ä‘Æ°á»£c sinh ra Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» Ä‘a cá»™ng tuyáº¿n trong linear regression báº±ng cÃ¡ch giáº£m kÃ­ch thÆ°á»›c cá»§a cÃ¡c biáº¿n tÆ°Æ¡ng quan quan Ä‘áº¿n nhau vÃ  mÃ´ hÃ¬nh thÃ´ng tin cÆ¡ báº£n cá»§a cÃ¡c biáº¿n Ä‘Ã³.

## PLS Ä‘á»‘i vá»›i bÃ i toÃ¡n multi-variate outcome

Má»™t Ä‘iá»ƒm Ä‘áº·c biá»‡t ná»¯a cá»§a thuáº­t toÃ¡n PLS lÃ  kháº£ nÄƒng model Ä‘Æ°á»£c bÃ i toÃ¡n vá»›i nhiá»u outcomes trong khi Ä‘Ã³ cÃ¡c thuáº­t toÃ¡n thá»‘ng kÃª hoáº·c machine learning khÃ´ng thá»ƒ lÃ m Ä‘Æ°á»£c Ä‘iá»u Ä‘Ã³ trá»±c tiáº¿p.

Máº·c dÃ¹ ta cÃ³ thá»ƒ xÃ¢y model cho má»—i biáº¿n, tuy nhiÃªn má»i yáº¿u tá»‘ cáº§n Ä‘Æ°á»£c giá»¯ trong model Ä‘áº·c biá»‡t lÃ  vá»›i cÃ¡c bÃ i toÃ¡n mÃ  cÃ¡c biáº¿n cÃ³ sá»± tÆ°Æ¡ng quan vá»›i nhau. HÆ¡n ná»¯a, viá»‡c giáº£i thÃ­ch mÃ´ hÃ¬nh multi-variate outcomes sáº½ khÃ¡c vá»›i viá»‡c giáº£i thÃ­ch nhiá»u mÃ´ hÃ¬nh univariate.

## So sÃ¡nh PLS vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c

TrÆ°á»›c khi Ä‘i vÃ o chi tiáº¿t phÆ°Æ¡ng phÃ¡p PLS, hÃ£y cÃ¹ng tÃ¬m hiá»ƒu xem lÃ½ do táº¡i sao vÃ  trong trÆ°á»ng há»£p nÃ o nÃªn dÃ¹ng PLS. So sÃ¡nh vá»›i vá»›i cÃ¡c thuáº­t toÃ¡n khÃ¡c Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» (1) cÃ³ nhiá»u biáº¿n phá»¥ thuá»™c (multiple outputs) vÃ  (2) cÃ³ nhiá»u biáº¿n Ä‘á»™c láº­p (inputs) tÆ°Æ¡ng quan vá»›i nhau

| PhÆ°Æ¡ng phÃ¡p                       | TrÆ°á»ng há»£p sá»­ dá»¥ng                                           | Giá»›i háº¡n                                                     |
| --------------------------------- | :----------------------------------------------------------- | ------------------------------------------------------------ |
| Multi-variate multiple regression | Nhiá»u inputs vÃ  nhiá»u outputs regression                     | KhÃ´ng cÃ³ kháº£ nÄƒng xá»­ lÃ½ hiá»‡n tÆ°á»£ng Ä‘a cá»™ng tuyáº¿n             |
| Principle component regression    | CÃ¡c inputs cÃ³ liÃªn quan Ä‘áº¿n nhau. Sá»­ dá»¥ng PCA trÆ°á»›c rá»“i cho vÃ o OLS model<br />PCR táº­p trung vÃ o giáº£m kÃ­ch thÆ°á»›c inputs báº±ng cÃ¡ch táº­p trung vÃ o phÆ°Æ¡ng sai (variance) | KhÃ´ng phÃ¹ há»£p vá»›i bÃ i toÃ¡n tÃ¬m má»‘i liÃªn há»‡ phá»¥ thuá»™c giá»¯a output vÃ  inputs |
| Canonical correlation analysis    | TÃ¬m má»‘i tÆ°Æ¡ng quan (correlation) giá»¯a 2 datasets<br />Thá»±c hiá»‡n báº±ng cÃ¡ch giáº£m kÃ­ch thÆ°á»›c cá»§a cáº£ 2 data vÃ  tÃ¬m ra cáº·p component nÃ o cÃ³ Ä‘á»™ tÆ°Æ¡ng quan cao nháº¥t | Táº­p trung vÃ o so sÃ¡nh má»©c Ä‘á»™ tÆ°Æ¡ng quan thay vÃ¬ hiá»‡p phÆ°Æ¡ng sai nhÆ° PLS |

## Partial Least Squares models

TrÆ°á»›c tiÃªn, Ä‘á»ƒ trÃ¡nh nháº§m láº«n vá»›i cÃ¡c Ä‘á»‹nh nghÄ©a trong cÃ¡c nghiÃªn cá»©u khÃ¡c, chÃºng ta cáº§n lÃ m rÃµ cÃ¡c Ä‘á»‹nh nghÄ©a hoáº·c khÃ¡i niá»‡m.

- **PLS regression**: cÅ©ng lÃ  má»™t há» cá»§a phÆ°Æ¡ng phÃ¡p PLS nhÆ°ng táº­p trung vÃ o bÃ i toÃ¡n mÃ  output lÃ  sá»‘
- **PLS discriminant analysis (PLS-DA)**: táº­p trung vÃ o bÃ i toÃ¡n vá»›i output lÃ  categories (bÃ i toÃ¡n phÃ¢n loáº¡i)
- **PLS1 vÃ  PLS2**: model vá»›i chá»‰ Má»˜T output vÃ  NHIá»€U outputs
- ***\*SIMPLS vs NIPALS:\**** lÃ  2 phÆ°Æ¡ng phÃ¡p Ä‘á»ƒ triá»ƒn khai PLS, trong Ä‘Ã³ SIMPLS Ä‘Æ°á»£c biáº¿t Ä‘áº¿n vá»›i kháº£ nÄƒng tÃ­nh toÃ¡n nhanh hÆ¡n vÃ  Ä‘Æ¡n giáº£n hÆ¡n so vá»›i ngÆ°á»i tiá»n nhiá»‡m lÃ  NIPALS.
- ***\*KernelPLS:\**** vÃ¬ PLS lÃ  biáº¿n thá»ƒ cá»§a phÆ°Æ¡ng phÃ¡p Linear regression nÃªn nÃ³ báº£n cháº¥t khÃ´ng cÃ³ kháº£ nÄƒng lÃ m viá»‡c vá»›i non-linear problems â†’ KernelPLS sinh ra Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» Ä‘Ã³
- **OPLS (orthogonal projects to latent structures)**: lÃ  má»™t báº£n nÃ¢ng cáº¥p cá»§a PLS, OPLS dá»… giáº£i thÃ­ch hÆ¡n. Náº¿u PLS chá»‰ phÃ¢n chia variability thÃ nh systemic vÃ  noise, thÃ¬ OPLS tiáº¿n thÃªm má»™t bÆ°á»›c báº±ng cÃ¡ch chia systemic variability thÃ nh predictive and orthogonal variability.

## Partial Least Squares Regression Example vá»›i `R`

á» pháº§n nÃ y, chÃºng ta sáº½ cÃ¹ng code PLS báº±ng ngÃ´n ngá»¯ R. Dataset sá»­ dá»¥ng lÃ  `meats` dataset, cÃ³ thá»ƒ tÃ¬m tháº¥y trong R-library.

Má»¥c tiÃªu cá»§a `meats` dataset lÃ  sá»­ dá»¥ng 100 data vá» Near-Infrared Ä‘á»ƒ dá»± Ä‘oÃ¡n thÃ nh pháº§n trong thá»‹t nhÆ° `water, fat, protein`. Náº¿u model cá»§a chÃºng ta cÃ³ thá»ƒ predict chÃ­nh xÃ¡c, ta sáº½ giÃºp quÃ¡ trÃ¬nh Ä‘o thÃ nh pháº§n trong thá»‹t nhanh hÆ¡n, chá»‰ cáº§n nháº­p input lÃ  cÃ³ thá»ƒ biáº¿t Ä‘Æ°á»£c káº¿t quáº£. LÃ½ do sá»­ dá»¥ng data nÃ y vÃ¬ nÃ³ cÃ³ nhiá»u Ä‘áº·c Ä‘iá»ƒm Ä‘á»ƒ sá»­ dá»¥ng PLS

- CÃ¡c inputs cÃ³ liÃªn quan Ä‘áº¿n nhau â†’ phÃ¹ há»£p cho viá»‡c giáº£m kÃ­ch thÆ°á»›c inputs
- Nhiá»u outputs vÃ  cÃ¡c output cÅ©ng cÃ³ sá»± tÆ°Æ¡ng quan vá»›i nhau

## Partial Least Squares Regression Example vá»›i `Python`

TrÆ°á»›c tiÃªn, ta sáº½ lÃ m viá»‡c vá»›i continuous data, Ä‘á»‘i vá»›i categorical data sáº½ Ä‘Æ°á»£c Ä‘á» cáº­p á»Ÿ pháº§n sau.

Trong `Python`, ta cÃ³ thá»ƒ import `meats` data tá»«

```python
import pandas as pd
import boto

# import the csv file directly from an s3 bucket
data = pd.read_csv('<https://raw.githubusercontent.com/hoanglinh96nthu/implement_algorithm/main/Regression_Algorithms/meats.csv>')
data = data.drop('Unnamed: 0', axis = 1)
data
```

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/24e513c3-8e44-4ac8-a9bd-713a0a1d8307/Untitled.png)

Sau khi Ä‘Ã£ cÃ³ data, ta tiáº¿n hÃ nh train vÃ  validate moadel báº±ng thÆ° viá»‡n `sklearn`

```python
from sklearn.model_selection import train_test_split
from sklearn.cross_decomposition import PLSRegression
from sklearn.metrics import mean_squared_error
water, fat, protein = [], [], []
for n_comp in range(1, 101):
    my_plsr = PLSRegression(n_components=n_comp, scale=True)
    my_plsr.fit(X_train, y_train)
    preds = my_plsr.predict(X_val)

    water.append(np.sqrt(mean_squared_error(y_val['water'].values, preds[:, 0])))
    fat.append(np.sqrt(mean_squared_error(y_val['fat'], preds[:, 1])))
    protein.append(np.sqrt(mean_squared_error(y_val['protein'], preds[:, 2])))

fig, ax = plt.subplots(nrows=1, ncols=3, sharex='all', sharey='all', figsize=(16, 4))
ax[0].plot(range(1, 101), water)
ax[1].plot(range(1, 101), fat)
ax[2].plot(range(1, 101), protein)

plt.suptitle('Partial Least Squares â€” plot for the number of components', fontsize=18)
fig.supxlabel('Number of components', fontsize=15)
fig.supylabel('RMSE value', fontsize=15)
plt.tight_layout()
plt.show()
```

![output.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3ca4d4ed-7d5b-4fde-831e-f407340c0860/output.png)

Tá»« hÃ¬nh váº½ trÃªn, ta cÃ³ thá»ƒ tháº¥y sá»‘ lÆ°á»£ng `component` cho ra káº¿t quáº£ `RMSE` nhá» nháº¥t náº±m trong khoáº£ng 20 â†’ 30. Ta cÅ©ng cÃ³ thá»ƒ visualize sá»± khÃ¡c nhau khi sá»‘ lÆ°á»£ng components thay Ä‘á»•i Ä‘á»‘i vá»›i káº¿t quáº£. VÃ¬ chÃºng ta cÃ³ tá»•ng cá»™ng lÃ  100 inputs nÃªn ta cÃ³ tá»•ng lÃ  100 há»‡ sá»‘ cho model regression.

![coefficient.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/34d9b7ad-01a1-44e6-9c2e-b0ba1d4ac120/coefficient.png)

CÃ³ thá»ƒ tháº¥y, khi sá»‘ lÆ°á»£ng components tÄƒng lÃªn, Ä‘á»™ phá»©c táº¡p cá»§a model cÅ©ng tÄƒng theo. Váº­y lÃ m tháº¿ nÃ o Ä‘á»ƒ biáº¿t Ä‘Æ°á»£c bao nhiÃªu `component` lÃ  tá»‘i Æ°u? Ta sáº½ sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p `Grid search` vÃ  tÃ­nh giÃ¡ trá»‹ `r-square`. Báº¡n Ä‘á»c cÃ³ thá»ƒ tÃ¬m hiá»ƒu thÃªm vá» `r-square` táº¡i [bÃ i viáº¿t nÃ y](https://www.notion.so/37daf433b45347338cc5afbe990bbe7a).

```python
from sklearn.metrics import r2_score

best_r2 = 0
best_ncmop = 0

for n_comp in range(1, 101):
    my_plsr = PLSRegression(n_components=n_comp, scale=True)
    my_plsr.fit(X_train, y_train)
    preds = my_plsr.predict(X_val)

    r2 = r2_score(preds, y_val)
    if r2 > best_r2:
        best_r2 = r2
        best_ncomp = n_comp

print(best_r2, best_ncmop)
```

`r-square` tá»‘t nháº¥t mÃ  chÃºng ta cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c lÃ  Ä‘iá»ƒm `r-square` lÃ  0,943 vá»›i giÃ¡ trá»‹ `ncomp` lÃ  16. Äá»ƒ xÃ¡c thá»±c láº§n cuá»‘i mÃ´ hÃ¬nh cá»§a chÃºng ta, hÃ£y xÃ¡c minh xem chÃºng ta cÃ³ Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»ƒm tÆ°Æ¡ng Ä‘Æ°Æ¡ng trÃªn táº­p dá»¯ liá»‡u thá»­ nghiá»‡m hay khÃ´ng:

```python
best_model = PLSRegression(n_components=best_ncmop, scale=True)
best_model.fit(X_train, Y_train)
test_preds = best_model.predict(X_test)
print(r2_score(Y_test, test_preds))
```

Äiá»ƒm `r-square` thu Ä‘Æ°á»£c lÃ  0,95628, cao hÆ¡n má»™t chÃºt so vá»›i káº¿t quáº£ trÃªn táº­p `validation`. ChÃºng ta cÃ³ thá»ƒ tá»± tin ráº±ng mÃ´ hÃ¬nh khÃ´ng bá»‹ overfitting vÃ  chÃºng ta Ä‘Ã£ tÃ¬m tháº¥y sá»‘ lÆ°á»£ng `components` phÃ¹ há»£p Ä‘á»ƒ táº¡o nÃªn má»™t mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng hiá»‡u quáº£.

<aside> ğŸ’¡ Náº¿u sai sá»‘ nÃ y cÃ³ thá»ƒ cháº¥p nháº­n Ä‘Æ°á»£c cho má»¥c Ä‘Ã­ch thá»­ nghiá»‡m thá»‹t, thÃ¬ chÃºng tÃ´i cÃ³ thá»ƒ tá»± tin thay tháº¿ phÃ©p Ä‘o nÆ°á»›c, cháº¥t bÃ©o vÃ  protein thá»§ cÃ´ng báº±ng phÃ©p Ä‘o hÃ³a há»c tá»± Ä‘á»™ng káº¿t há»£p vá»›i CÃ´ng cá»¥ há»“i quy PLS nÃ y. Há»“i quy PLS sau Ä‘Ã³ sáº½ phá»¥c vá»¥ Ä‘á»ƒ chuyá»ƒn Ä‘á»•i cÃ¡c phÃ©p Ä‘o hÃ³a há»c thÃ nh Æ°á»›c tÃ­nh hÃ m lÆ°á»£ng nÆ°á»›c, cháº¥t bÃ©o vÃ  protein.

</aside>

------

